import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client
from dotenv import load_dotenv
import time
 
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv() 

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

BASE_URL = "https://www.10000recipe.com"


def safe_get(url, delay=5):
    """ì‘ë‹µ ì˜¬ ë•Œê¹Œì§€ ë¬´í•œ ì¬ì‹œë„í•˜ëŠ” GET ìš”ì²­"""
    while True:
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response
        except (requests.exceptions.RequestException, requests.exceptions.ConnectionError) as e:
            print(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {e}\nâ³ {delay}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘...")
            time.sleep(delay)


def get_recipe_links():
    """ëª¨ë“  í˜ì´ì§€ì—ì„œ ë ˆì‹œí”¼ ë§í¬ í¬ë¡¤ë§"""
    page = 1
    all_links = []

    while True:
        url = f"{BASE_URL}/recipe/list.html?order=reco&page={page}"
        response = safe_get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        links = [BASE_URL + a["href"] for a in soup.select(".common_sp_list_li a.common_sp_link")]

        if not links:  # ë” ì´ìƒ ë§í¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            print(f"âœ… ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {page - 1} í˜ì´ì§€)")
            break

        all_links.extend(links)
        print(f"ğŸ“Œ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ, í˜„ì¬ê¹Œì§€ {len(all_links)}ê°œ ìˆ˜ì§‘")
        page += 1
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    return all_links


def get_saved_urls():
    """Supabaseì— ì´ë¯¸ ì €ì¥ëœ ë ˆì‹œí”¼ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    saved_urls = []
    page = 0

    while True:
        response = supabase.table("recipes_db").select("url").range(page * 1000, (page + 1) * 1000 - 1).execute()
        if not response.data:
            break
        saved_urls.extend([item["url"] for item in response.data])
        page += 1

    return set(saved_urls)


def get_recipe_details(recipe_url):
    """ê°œë³„ ë ˆì‹œí”¼ ìƒì„¸ì •ë³´ í¬ë¡¤ë§"""
    response = safe_get(recipe_url)
    soup = BeautifulSoup(response.text, "html.parser")

    try:
        title = soup.select_one(".view2_summary h3").text.strip()
        image = soup.select_one(".centeredcrop img")["src"]
        ingredients = [i.text.strip() for i in soup.select(".ready_ingre3 ul li")]
        steps = [s.text.strip() for s in soup.select(".view_step .media-body")]

        return {
            "title": title,
            "image": image,
            "ingredients": ", ".join(ingredients),
            "steps": " | ".join(steps),
            "url": recipe_url
        }
    except AttributeError:
        print(f"âŒ ë ˆì‹œí”¼ íŒŒì‹± ì‹¤íŒ¨: {recipe_url}")
        return None


def save_to_supabase(recipe):
    """Supabaseì— ë°ì´í„° ì €ì¥"""
    response = supabase.table("recipes_db").insert(recipe).execute()
    if response.data:
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {recipe['title']}")
    else:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {recipe['title']} - {response.error}")


def main():
    """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
    recipe_links = get_recipe_links()
    saved_urls = get_saved_urls()

    print(f"âœ… ì´ë¯¸ ì €ì¥ëœ ë ˆì‹œí”¼ ìˆ˜: {len(saved_urls)}")
    print(f"ğŸ“¦ ì´ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(recipe_links)}")

    # ì¤‘ë³µ ì œê±°
    new_links = [link for link in recipe_links if link not in saved_urls]
    print(f"ğŸš€ ìƒˆë¡œ ì €ì¥í•  ë ˆì‹œí”¼ ìˆ˜: {len(new_links)}")

    for idx, link in enumerate(new_links):
        print(f"\nğŸ“Œ ({idx + 1}/{len(new_links)}) {link} í¬ë¡¤ë§ ì¤‘...")
        recipe = get_recipe_details(link)

        if recipe:
            save_to_supabase(recipe)

        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€


if __name__ == "__main__":
    main()